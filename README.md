
# Pollinations AI Multi-Endpoint Proxy Server

This project provides a customizable Node.js proxy server that exposes OpenAI-compatible API endpoints. It allows you to use services like Pollinations.ai for image generation and Google Gemini (or a custom service) for chat completions through a unified, OpenAI-standard API.

This server is generated by the **Pollinations AI Proxy Server Generator**.

## Features

*   **OpenAI-Compatible API:** Use any client application that supports the OpenAI API standard.
*   **Image Generation:** The `/v1/images/generations` endpoint is powered by [Pollinations.ai](https://pollinations.ai/), offering robust and free image creation.
*   **Flexible Chat Completions:** The `/v1/chat/completions` endpoint can be configured to:
    *   Use **Google Gemini** for powerful language model capabilities.
    *   Proxy requests to a **custom OpenAI-compatible URL** (e.g., a local model running with LM Studio, Jan, or Ollama).
*   **Configurable TTS:** Includes a configurable `/v1/audio/speech` (Text-to-Speech) endpoint that can either act as a placeholder or proxy requests to a custom TTS service URL.
*   **Easy Configuration:** A simple web interface allows you to enable/disable endpoints, configure settings, and securely input your API keys before generating the server code.

## Setup and Usage

Follow these steps to get your local proxy server up and running.

### Step 1: Generate and Save the Server Code

1.  Use the web-based generator to configure your desired server settings (port, image dimensions, active endpoints, API keys, etc.).
2.  Click the "Copy Code" button to copy the generated Node.js server code.
3.  Save this code into a new file named `proxy-server.mjs` in a local directory on your computer.

### Step 2: Install Dependencies

1.  Open your terminal or command prompt.
2.  Navigate to the directory where you saved `proxy-server.mjs`.
3.  Install the necessary Node.js packages by running the command provided in the generator's "Instructions" section. The command will look similar to this:

    ```bash
    # Basic dependencies are always needed
    npm install express cors

    # Add '@google/genai' if you use Gemini
    npm install @google/genai

    # Add 'node-fetch@2' if you use the custom URL proxy for chat or TTS
    npm install node-fetch@2
    ```
    *Note: The generator provides the exact `npm install` command you need based on your configuration.*

### Step 3: Run the Server

Start the proxy server by running the following command in your terminal:

```bash
node proxy-server.mjs
```

You should see a confirmation message in the console indicating that the server is running and which endpoints are active.

```
Server started. OpenAI-compatible endpoint available at http://localhost:8111/v1
-> Image Generation: POST http://localhost:8111/v1/images/generations (ACTIVE)
-> Chat Completions: POST http://localhost:8111/v1/chat/completions (ACTIVE - Using Gemini)
-> TTS: POST http://localhost:8111/v1/audio/speech (ACTIVE - Using Custom URL)
```

### Step 4: Configure Your Client Application

Now, you can point any OpenAI-compatible application to your local proxy.

1.  In your application's settings, find the "API Endpoint" or "Base URL" configuration.
2.  Set the URL to your local proxy server, for example: `http://localhost:8111/v1`. (Replace `8111` with the port you configured).
3.  For the API Key, you can typically enter any value. The proxy itself handles authentication with the backend services (like Gemini). If you are proxying to a custom URL that requires an API key, the proxy will use the key you provided in the generator or forward the `Authorization` header from your client application if you left the key field blank.
