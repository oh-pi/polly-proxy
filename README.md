# Pollinations AI Multi-Endpoint Proxy Server

This project provides a customizable Node.js proxy server that exposes OpenAI-compatible API endpoints. It allows you to use services like Pollinations.ai for image generation and Google Gemini (or a custom service) for chat completions through a unified, OpenAI-standard API.

This server is generated by the **Pollinations AI Proxy Server Generator**.

## Features

*   **OpenAI-Compatible API:** Use any client application that supports the OpenAI API standard.
*   **Image Generation:** The `/v1/images/generations` endpoint is powered by [Pollinations.ai](https://pollinations.ai/), offering robust and free image creation.
*   **Flexible Chat Completions:** The `/v1/chat/completions` endpoint can be configured to:
    *   Use **Google Gemini** for powerful language model capabilities.
    *   Proxy requests to a **custom OpenAI-compatible URL** (e.g., a local model running with LM Studio, Jan, or Ollama).
*   **Placeholder Endpoints:** Includes an activatable placeholder for `/v1/audio/speech` (Text-to-Speech).
*   **Easy Configuration:** A simple web interface allows you to enable/disable endpoints and configure settings before generating the server code.

## Setup and Usage

Follow these steps to get your local proxy server up and running.

### Step 1: Generate and Save the Server Code

1.  Use the web-based generator to configure your desired server settings (port, image dimensions, active endpoints, etc.).
2.  Click the "Copy Code" button to copy the generated Node.js server code.
3.  Save this code into a new file named `proxy-server.js` in a local directory on your computer.

### Step 2: Set Up Environment Variables (If using Google Gemini)

If you have configured the Chat Completions endpoint to use Google Gemini, you need to provide your API key.

1.  In the same directory where you saved `proxy-server.js`, create a new file named `.env`.
2.  Add your Google Gemini API key to this file:

    ```env
    API_KEY="YOUR_GEMINI_API_KEY"
    ```

### Step 3: Install Dependencies

1.  Open your terminal or command prompt.
2.  Navigate to the directory where you saved `proxy-server.js`.
3.  Install the necessary Node.js packages by running the command provided in the generator's "Instructions" section. The command will look similar to this:

    ```bash
    # Basic dependencies are always needed
    npm install express cors

    # Add 'dotenv' and '@google/genai' if you use Gemini
    npm install dotenv @google/genai

    # Add 'node-fetch@2' if you use the custom URL proxy
    npm install node-fetch@2
    ```
    *Note: The generator provides the exact `npm install` command you need based on your configuration.*

### Step 4: Run the Server

Start the proxy server by running the following command in your terminal:

```bash
node proxy-server.js
```

You should see a confirmation message in the console indicating that the server is running and which endpoints are active.

```
Server started. OpenAI-compatible endpoint available at http://localhost:8111/v1
-> Image Generation: POST http://localhost:8111/v1/images/generations (ACTIVE)
-> Chat Completions: POST http://localhost:8111/v1/chat/completions (ACTIVE - Using Gemini)
-> TTS: POST http://localhost:8111/v1/audio/speech (Placeholder)
```

### Step 5: Configure Your Client Application

Now, you can point any OpenAI-compatible application to your local proxy.

1.  In your application's settings, find the "API Endpoint" or "Base URL" configuration.
2.  Set the URL to your local proxy server, for example: `http://localhost:8111/v1`. (Replace `8111` with the port you configured).
3.  For the API Key, you can typically enter any value. The proxy itself handles authentication with the backend services (like Gemini, which uses the `.env` file). If you are proxying to a custom URL that requires an API key, the proxy will forward the `Authorization` header from your client application.

Your application will now send its API requests to your local proxy, which will route them to the appropriate backend service.
